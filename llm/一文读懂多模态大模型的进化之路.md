随着注意力机制和Transformer架构的引入，我们见证了处理多种模态信息的模型在规模和能力上的飞跃。这一进步主要得益于这些技术的通用性和灵活性。最初，这些技术被应用于语言特定的模型，但很快就扩展到了视觉处理的支持，并最终发展为能够整合多种模态的模型。

特别是，大语言模型（LLMs）的复杂性和它们对上下文学习的能力，激励研究者们将这些模型的应用范围扩展到输入和输出的多模态领域。这种扩展导致了如GPT-4V和Gemini等尖端模型的开发，它们在多个领域展现了最先进的性能。



多模态大语言模型（MLLMs）的发展涉及将视觉和语言的单模态架构合并，通过视觉到语言的适配器建立有效的连接，并设计创新的训练方法。这些方法对于确保模态之间的对齐和准确遵循指令至关重要。在快速发布新模型的背景下，我们的目标是提供MLLM领域的全面概述，重点关注利用视觉模态的模型。这一概述既是对当前状态的更新，也是未来发展的灵感来源。



**论文标题**:  
The (R)Evolution of Multimodal Large Language Models ：A Survey



**论文链接**:  
[_https://arxiv.org/pdf/2402.12451.pdf_](https://arxiv.org/pdf/2402.12451.pdf)

## MLLMs的发展
MLLMs的发展路径与LLMs相似，Flamingo是首个在视觉语言领域大规模探索上下文学习的模型。随后，视觉指令调整迅速成为多模态领域最突出的训练范式，以及使用PEFT技术对LLM进行微调。如下图所示，任何MLLM至少包含三个组件：作为与用户交互的接口的LLM主干，一个（或多个）视觉编码器（Visual Encoder），以及一个或多个视觉到语言适配器模块（Adapter）。

流行的LLM主干选择通常属于LLaMA家族，鉴于它们的权重是免费可获取的，它们仅在公共数据上进行了训练，并且为了适应不同的用例，它们拥有不同的大小。此外，它们的衍生版本也很受欢迎，例如Alpaca和Vicuna。前者在使用GPT-3编写的指令上微调LLaMA，而后者利用用户与ChatGPT的共享对话。其他选择包括OPT、Magneto、MPT以及经过指令调整或多语言版本的T5，这是一个为多个任务预训练的编码器-解码器语言模型。



![](https://mmbiz.qpic.cn/mmbiz_jpg/5fknb41ib9qH4saDBHNyl4UpQ91yiblMW5xguAwFB7XGtXvRibCc9ZqVrUPDWZPgiaEVK0EiaGVw2gOK1k3s5MPeoPw/640?wx_fmt=jpeg&from=appmsg)



## MLLMs的架构概述


**1. 语言模型的核心作用与用户接口**



多模态大语言模型（MLLMs）是在传统大语言模型（LLMs）的基础上发展而来，旨在处理和理解多种模态的数据，如文本和图像。LLMs的核心作用在于其强大的语言理解和生成能力，这使得它们能够在没有明确任务指示的情况下，通过上下文学习（in-context learning）来执行各种语言任务。MLLMs继承了这些能力，并通过用户接口与用户进行交互，用户可以通过对话形式提出问题或指令，模型则能够理解并生成相应的回答或执行指定的任务。



**2. 视觉编码器的重要性与常用模型**



视觉编码器在MLLMs中扮演着至关重要的角色，它负责将视觉信息转换为模型能够理解的特征表示。这些特征随后会被送入语言模型，与文本信息结合进行处理。常用的视觉编码器模型包括基于Vision Transformer（ViT）的CLIP和OpenCLIP模型，它们通过对齐图像和文本嵌入来训练，以此捕获视觉内容的丰富信息。此外，EVA-CLIP模型[1]也被用于训练MLLMs，它通过重建遮蔽的图像-文本对齐视觉特征来提供有效的视觉编码（下图）。



![](https://mmbiz.qpic.cn/mmbiz_png/5fknb41ib9qH4saDBHNyl4UpQ91yiblMW5WagdB4LFnqAHrsmhkynz11MkiaROCwXAU577lNC7TqFHmzJAVNdwxRA/640?wx_fmt=png&from=appmsg)



**3. 视觉到语言适配器的作用与类型**



视觉到语言适配器是连接视觉编码器和语言模型的关键组件，它使得不同模态的信息能够在同一模型中融合和交互。适配器的类型多样，从简单的线性层或多层感知机（MLP）到更复杂的基于Transformer的Q-Former模型[2]，以及增加的条件交叉注意力层。这些适配器通过不同的机制将视觉特征映射到文本空间，从而实现跨模态的信息融合（下图是Q-Former使用一组可学习的查询向量从冻结的图像编码器中提取视觉特征，在这里它为LLM提供最有用的视觉特征，以便LLM输出期望的文本。）。



![](https://mmbiz.qpic.cn/mmbiz_png/5fknb41ib9qH4saDBHNyl4UpQ91yiblMW5sO4baERT6Yzia48icfL8xUbo777Odia5Ueialk3UmEMvN276uSaWJGm7ew/640?wx_fmt=png&from=appmsg)



## MLLMs的训练方法与数据


**1. 单阶段与双阶段训练的对比**



MLLMs的训练通常涉及单阶段或双阶段过程。单阶段训练中，模型通常使用图像-文本对进行联合训练，同时处理视觉知识和文本指令。而双阶段训练首先关注将图像特征与文本嵌入空间对齐，在第二阶段则进一步提升模型的多模态对话能力。不同的MLLMs采用不同的训练策略，以适应特定的任务和性能要求。



**2. 训练数据的来源与特点**



MLLMs的训练数据来自多个来源，包括公开的图像-文本数据集如LAION-2B[3]、LAION-400M[4]、Conceptual Captions[5]和COYO-700M[6]等。这些数据集提供了大量的图像和对应的描述，用于训练模型的视觉理解能力。此外，还有专门为视觉指令调整而设计的数据集，如LLaVA-Instruct[7]，它扩展了COCO数据集，加入了由GPT-4生成的指令。这些数据集的多样性和丰富性对于训练具有强大视觉理解和生成能力的MLLMs至关重要。



## MLLMs在视觉任务中的应用


**1. 视觉理解任务的范畴**



视觉理解任务是多模态大语言模型（MLLMs）的关键应用领域之一。这些任务通常要求模型能够理解和解释视觉内容，包括图像和视频。视觉理解的范畴广泛，涵盖了从基本的图像分类和目标检测到更复杂的场景解析、视觉问答（VQA）、图像字幕生成和视觉对话等任务。例如，在视觉问答任务中，模型需要根据图像内容回答相关问题，这不仅要求模型具备强大的视觉感知能力，还要求其能够理解和处理自然语言。



**2. 视觉定位与图像生成的进展**



在视觉定位方面，MLLMs已经能够实现精确的目标检测和定位。例如，通过结合视觉编码器和语言模型，MLLMs可以识别图像中的特定对象，并在对话中提供关于这些对象的信息。在图像生成方面，MLLMs的进展同样显著。它们不仅可以生成与文本描述相匹配的图像，还可以进行图像编辑，如改变图像中对象的颜色或形状。这些能力的提升得益于先进的视觉编码器和适配器模块的开发，以及针对特定视觉任务的训练数据集和评估基准的编制。



## 面向视频、多模态和特定领域的MLLMs


**1. 视频理解能力的增强**



随着研究的深入，MLLMs在视频理解方面的能力也得到了显著提升。这些模型可以独立处理视频帧，提取帧级特征，并通过池化机制或基于Q-Former的解决方案结合这些特征。这使得MLLMs能够处理视频对话、视频问答和视频字幕生成等任务。此外，一些模型还结合了音频特征，进一步丰富了视频序列的表示。



**2. 能处理多种模态的模型**



上文讨论的几乎所有模型都将单一模态作为输入处理。然而，有大量研究工作集中在设计能够有效处理多种模态的有效解决方案上。这通常是通过使用如Q-Former和Perceiver[8]这样的Transformer模块来对齐多模态特征，或者通过利用ImageBind[9]来有效提取本质上是多模态的特征并进行联合空间嵌入（下图），图像、视频和音频是最常见的处理模态。此外，一些工作也有效地编码了3D数据和惯性测量单元（IMU）传感器信号。



![](https://mmbiz.qpic.cn/mmbiz_png/5fknb41ib9qH4saDBHNyl4UpQ91yiblMW5yR9CKts8eUTKsMaWypZ0K19XBib5fNpxB1swwGnou0xVdZ9jWDbIscg/640?wx_fmt=png&from=appmsg)



除了这些能处理多模态输入的解决方案，也有像NExT-GPT[10]和Unified-IO 2[11]这样的能够生成不同模态输出的解决方案。下图是NExT-GPT通过将LLM与多模态适配器和扩散解码器连接起来， 实现了通用多模态理解和任意模态间的输入和输出。



![](https://mmbiz.qpic.cn/mmbiz_png/5fknb41ib9qH4saDBHNyl4UpQ91yiblMW58fiaUgyeoXFZJolx5dzvAQ9wpO70caD9nc0EXqc3I2RbfYTb7y1pggw/640?wx_fmt=png&from=appmsg)



**3. 针对特定应用领域的模型定制**



除了通用的视觉理解任务，MLLMs也被定制用于特定领域的应用。这些领域特定的MLLMs通常是在预训练的LLM基础上进行训练，或者使用特定领域的数据对现有MLLM进行微调。例如，已经开发了针对文档分析、医疗视觉学习和自动驾驶等领域的MLLMs。这些模型不仅能够处理通用的视觉输入，还能够理解和执行与特定领域相关的复杂任务，如信息提取、图表分析和交通状况理解。

## MLLMs的挑战与未来方向
在多模态大语言模型（MLLMs）的发展过程中，研究者们面临着一系列挑战，同时也在探索未来的发展方向。以下是目前MLLMs面临的一些主要挑战以及可能的解决策略。



**1. 减少幻觉现象的策略**



MLLMs在生成长文本描述时，常常会出现幻觉现象，即模型生成与图像内容不符的描述。为了解决这一问题，研究者们提出了多种策略。例如，通过引入对抗性样本来训练模型，使其能够更好地区分相关与不相关的内容。此外，还可以通过增加模型的视觉理解能力，例如使用更强大的视觉编码器，或者通过两阶段训练方法，在第一阶段让视觉编码器可训练，以增强模型对视觉信息的捕捉能力。



**2. 防止生成有害和有偏见内容的方法**



由于MLLMs通常使用从网络上收集的大量数据进行训练，这些数据可能包含有害或有偏见的内容。为了防止模型生成不当内容，研究者们正在探索不同的方法。一种方法是在训练过程中引入安全和公平性的约束，以确保模型不会生成不适当的内容。另一种方法是使用更加精细的数据清洗和筛选技术，以减少训练数据中的有害内容。



**3. 降低计算负担的可能途径**



从下图可以看到各种MLLMs训练所需的GPU小时数，说明MLLMs训练和部署需要大量的计算资源，这限制了它们的可访问性和可持续性。为了降低计算负担，研究者们正在探索多种途径。例如，使用参数高效的微调技术（PEFT），只对模型的一小部分参数进行更新，从而减少训练成本。此外，还可以通过模型压缩和量化技术来减少模型的大小和运行时的计算需求。



![](https://mmbiz.qpic.cn/mmbiz_png/5fknb41ib9qH4saDBHNyl4UpQ91yiblMW5ia9AOEG5icuwUVzAmbDWrDiaCVaf09KeLqRlc2ePFCQEIicuH6DlQpAfUA/640?wx_fmt=png&from=appmsg)



## 总结：MLLMs的当前状态与未来展望
MLLMs作为一种新兴的技术，已经在多模态理解和生成任务中展现出了巨大的潜力。然而，它们仍然面临着一系列挑战，包括如何减少幻觉现象、防止生成有害内容以及降低计算成本。未来的研究将需要在提高模型性能的同时，解决这些挑战，以实现更广泛的应用和更好的用户体验。随着技术的不断进步和研究的深入，我们有理由相信MLLMs将在未来的人工智能领域扮演更加重要的角色。

## 参考资料
[1] Yuxin Fang, Wen Wang, Binhui Xie, Quan Sun, Ledell Wu, Xinggang Wang, Tiejun Huang, Xinlong Wang, and Yue Cao. 2023. Eva: Exploring the limits of masked visual representation learning at scale. In CVPR.  
[2] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. 2023f. BLIP-2: Bootstrapping Language-Image Pretraining with Frozen Image Encoders and Large Language Models. arXiv preprint arXiv:2301.12597.  
[3] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. 2022. LAION-5B: An open large-scale dataset for training next generation image-text models. In NeurIPS.  
[4] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. 2021. LAION-400M: Open Dataset of CLIPFiltered 400 Million Image-Text Pairs. In NeurIPS Workshops.  
[5] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. 2018. Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In ACL.  
[6] Minwoo Byeon, Beomhee Park, Haecheon Kim, Sungjun Lee, Woonhyuk Baek, and Saehoon Kim. 2022. COYO-700M: Image-Text Pair Dataset.  
[7] Haotian Liu, Chunyuan Li, QingyangWu, and Yong Jae Lee. 2023e. Visual Instruction Tuning. In NeurIPS.  
[8] Zijia Zhao, Longteng Guo, Tongtian Yue, Sihan Chen, Shuai Shao, Xinxin Zhu, Zehuan Yuan, and Jing Liu. 2023d. ChatBridge: Bridging Modalities with Large Language Model as a Language Catalyst. arXiv preprint arXiv:2305.16103.  
[9] Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala, Armand Joulin, and Ishan Misra. 2023. ImageBind: One Embedding Space To Bind Them All. In CVPR.  
[10] Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-Seng Chua. 2023b. NExT-GPT: Any-to-Any Multimodal LLM. arXiv preprint arXiv:2309.05519.  
[11] Jiasen Lu, Christopher Clark, Sangho Lee, Zichen Zhang, Savya Khosla, Ryan Marten, Derek Hoiem, and Aniruddha Kembhavi. 2023a. Unified-IO 2: Scaling Autoregressive Multimodal Models with Vision, Language, Audio, and Action. arXiv preprint arXiv:2312.17172.

