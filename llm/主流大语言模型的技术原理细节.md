<font style="color:rgb(52, 73, 94);">1.比较 LLaMA、ChatGLM、Falcon 等大语言模型的细节：tokenizer、位置编码、Layer Normalization、激活函数等。</font>

<font style="color:rgb(52, 73, 94);">2. 大语言模型的分布式训练技术：数据并行、张量模型并行、流水线并行、3D 并行、零冗余优化器 ZeRO、CPU 卸载技术 ZeRo-offload、混合精度训练、激活重计算技术、Flash Attention、Paged Attention。</font>

<font style="color:rgb(52, 73, 94);">3. 大语言模型的参数高效微调技术：prompt tuning、prefix tuning、adapter、LLaMA-adapter、 LoRA。</font>

### 0. 大纲
![](https://cdn.nlark.com/yuque/0/2023/png/35026057/1694707909851-7f7ae476-fbed-45a0-adc9-805d3ed287be.png)  
 

### 1. 大语言模型的细节
#### 1.0 transformer 与 LLM
![](https://mmbiz.qpic.cn/sz_mmbiz_png/j3gficicyOvasOHYuGEic5USJwXGgWfxiaIXRic6D6bGEEnOKH0G5Q7HgbTnbMUBYBzPX4w7eG66tDTn1eZsqeYVWow/640?wx_fmt=png)



#### 1.1 模型结构
![](https://mmbiz.qpic.cn/sz_mmbiz_png/j3gficicyOvasOHYuGEic5USJwXGgWfxiaIXGScJhkXmqlo0fOUwZ8FcXzGOwtz4KjWvqTCh7flHXfNOMicic7lia3lzA/640?wx_fmt=png)



#### 1.2 训练目标
![](https://mmbiz.qpic.cn/sz_mmbiz_png/j3gficicyOvasOHYuGEic5USJwXGgWfxiaIXKuwicIp9UTA2NvVlNl0sQHWtFTAFBZMaxkQZFrOPl8afIGqx9BpSdww/640?wx_fmt=png)



#### 1.3 tokenizer
![](https://mmbiz.qpic.cn/sz_mmbiz_png/j3gficicyOvasOHYuGEic5USJwXGgWfxiaIXApgq5BsWtSTXQxDlsE32YSTqkeONHJTMYicdLt9BnXsBwhjLQcS5VMA/640?wx_fmt=png)



#### 1.4 位置编码


![](https://mmbiz.qpic.cn/sz_mmbiz_png/j3gficicyOvasOHYuGEic5USJwXGgWfxiaIXc07vV56M7mejh4ndE9rckQdNUibnDblicMgcNOTKHTxaz97fU2IIodibA/640?wx_fmt=png)



#### 1.5 层归一化


![](https://mmbiz.qpic.cn/sz_mmbiz_png/j3gficicyOvasOHYuGEic5USJwXGgWfxiaIXPYPskvwcsoMQf3FM8rfHxf8QSGunTVq7FibocrF146J7ulia0vOlVCjw/640?wx_fmt=png)



#### 1.6 激活函数


![](https://mmbiz.qpic.cn/sz_mmbiz_png/j3gficicyOvasOHYuGEic5USJwXGgWfxiaIXytT7VEiaic8lhXb6e0gmlxOCib5XIVqyxpLfET09LicWT89s8NS2jUnmMQ/640?wx_fmt=png)



#### 1.7 Multi-query Attention 与 Grouped-query Attention


![](https://mmbiz.qpic.cn/sz_mmbiz_png/j3gficicyOvasOHYuGEic5USJwXGgWfxiaIXN7hpicqooQIX7ib7TAnsFz3Esh56yP4Vkuj6ic4rTtUTPpHAKxYLsTicGQ/640?wx_fmt=png)



#### 1.8 并行 transformer block


![](https://mmbiz.qpic.cn/sz_mmbiz_png/j3gficicyOvasOHYuGEic5USJwXGgWfxiaIXDkCkTKkPndwjibvTAxibeEUPa6RbMDX2sbRf1tibBtRhD31rRIjh1LLsQ/640?wx_fmt=png)



#### 1.9 总结-训练稳定性
![](https://mmbiz.qpic.cn/sz_mmbiz_png/j3gficicyOvasOHYuGEic5USJwXGgWfxiaIXtHn7U83Am6G0T79U4ibs3EYxYagNF4Ebjruc65Q9ED5wxHduykFLSibg/640?wx_fmt=png)



### 2. LLM 的分布式预训练
![](https://mmbiz.qpic.cn/sz_mmbiz_png/j3gficicyOvasOHYuGEic5USJwXGgWfxiaIXTdIUdswAXntq81j5DPzFRZnGic5SpP0LGjd7ibr2yLM27GTTazHLrvxQ/640?wx_fmt=png)



#### 2.0 点对点通信与集体通信


![](https://mmbiz.qpic.cn/sz_mmbiz_png/j3gficicyOvasOHYuGEic5USJwXGgWfxiaIXoJuRsnxBuAYIfJic66vPGfqCNXRxibPdibAtRqicticMdiaLVBdYbOl3Dq7g/640?wx_fmt=png)



#### 2.1 数据并行


![](https://mmbiz.qpic.cn/sz_mmbiz_png/j3gficicyOvasOHYuGEic5USJwXGgWfxiaIXib4iciaS7aA5tUAtR0UkEgIk8nBxKg2Dmg0VVpW8ouq3VJP9D22ac3Zwg/640?wx_fmt=png)



#### 2.2 张量并行


![](https://mmbiz.qpic.cn/sz_mmbiz_png/j3gficicyOvasOHYuGEic5USJwXGgWfxiaIXicib8UcmqX1qIQdvYfQDAPx6ODWWiavytL1ibjSNTrff1T5QFKVvu4I2OA/640?wx_fmt=png)



![](https://mmbiz.qpic.cn/sz_mmbiz_png/j3gficicyOvasOHYuGEic5USJwXGgWfxiaIXO5fGxMONvZZLq2XHiboESXHzWM2evQpk5RDJvlBNRibuSxSEKCawpAQw/640?wx_fmt=png)



### 2.3 流水线并行


![](https://mmbiz.qpic.cn/sz_mmbiz_png/j3gficicyOvasOHYuGEic5USJwXGgWfxiaIXFyqXLwzul3WX3DGp9FuIXMN0LlSMaicbNCf1FVsmnbO2nETaXUMjMgA/640?wx_fmt=png)



#### 2.4 3D 并行


![](https://mmbiz.qpic.cn/sz_mmbiz_png/j3gficicyOvasOHYuGEic5USJwXGgWfxiaIXQyCa3bcKElUBJuLfB05M4qqOrEDUkgFDkGqw3mVZNwoBwHqwKcXeFA/640?wx_fmt=png)



#### 2.5 混合精度训练


![](https://mmbiz.qpic.cn/sz_mmbiz_png/j3gficicyOvasOHYuGEic5USJwXGgWfxiaIXPkLxW2NA4ggcDLhSuibC5PNc9gNCFvMV4de5mXfry5fnibictiaAHIgHtQ/640?wx_fmt=png)



#### 2.6 激活重计算


![](https://mmbiz.qpic.cn/sz_mmbiz_png/j3gficicyOvasOHYuGEic5USJwXGgWfxiaIX9zGxG90OrUIhwkfBdYaIn2aEib9LibGrt3CicLjo5kTSY8atoTIDsrzSg/640?wx_fmt=png)



#### 2.7 ZeRO，零冗余优化器


![](https://mmbiz.qpic.cn/sz_mmbiz_png/j3gficicyOvasOHYuGEic5USJwXGgWfxiaIXnQxU9kdlz0wEsiaJ3luPHPHVbwRnMicuuQAibct97f4fsSw29DzS3aZYw/640?wx_fmt=png)



#### 2.8 CPU-offload，ZeRO-offload


![](https://mmbiz.qpic.cn/sz_mmbiz_png/j3gficicyOvasOHYuGEic5USJwXGgWfxiaIXszAnnsuIDm4cebia41oFtt9URetEMrpBxIqKoN2aR6lhnlYGYFTEnvQ/640?wx_fmt=png)



#### 2.9 Flash Attention


![](https://mmbiz.qpic.cn/sz_mmbiz_png/j3gficicyOvasOHYuGEic5USJwXGgWfxiaIXuy8BErQs0pq6HtARIFlpaPNia8K1iaWBmY3IVkLBC5ib5kg5Jo0JyrR0w/640?wx_fmt=png)



#### 2.10 vLLM: Paged Attention


![](https://mmbiz.qpic.cn/sz_mmbiz_png/j3gficicyOvasOHYuGEic5USJwXGgWfxiaIXTDG7fibVVNE2cBh9uoaeicoOSMXCwnyBOrPjGMZcxvibREl6mIcvTZDxA/640?wx_fmt=png)



### 3. LLM 的参数高效微调


#### 3.0 为什么进行参数高效微调？


![](https://mmbiz.qpic.cn/sz_mmbiz_png/j3gficicyOvasOHYuGEic5USJwXGgWfxiaIXlTicrThIGfEoIbNkEU9wrJLvic32WcuIeL5I3wXJBSPKWHwibP01v95ug/640?wx_fmt=png)



#### 3.1 prompt tuning


![](https://mmbiz.qpic.cn/sz_mmbiz_png/j3gficicyOvasOHYuGEic5USJwXGgWfxiaIXdagMdaAya7umnPjmrBdibWPNSDxApUIYqe7cicP3iapexWEKqRbfqFsdg/640?wx_fmt=png)



#### 3.2 prefix tuning


#### ![](https://mmbiz.qpic.cn/sz_mmbiz_png/j3gficicyOvasOHYuGEic5USJwXGgWfxiaIXObXV0RGnS1UVPok7QM4uOH5zibOYCYEF2yTwJpFjQ5ojLN2fIdNBoqA/640?wx_fmt=png)


#### 3.3 adapter


![](https://mmbiz.qpic.cn/sz_mmbiz_png/j3gficicyOvasOHYuGEic5USJwXGgWfxiaIXgNGmTSLdPaNbqmEGzInuVq028Lv3Z1ibypIX1kzdqBUXsicwGIXOShRg/640?wx_fmt=png)



#### 3.4 LLaMA adapter


![](https://mmbiz.qpic.cn/sz_mmbiz_png/j3gficicyOvasOHYuGEic5USJwXGgWfxiaIXxRcXbGF0tI1L0ibstPhRLibkXPhOZ2GHhxzHHHlVcqxca1J4trKqrDKg/640?wx_fmt=png)



#### 3.5 LoRA


![](https://mmbiz.qpic.cn/sz_mmbiz_png/j3gficicyOvasOHYuGEic5USJwXGgWfxiaIXlGewYtJO99MxGEVS09Mf58tbDz3aOmUD2uwfJXRHaelHpRib9H2bUzQ/640?wx_fmt=png)



#### 3.6 实验比较


![](https://mmbiz.qpic.cn/sz_mmbiz_png/j3gficicyOvasOHYuGEic5USJwXGgWfxiaIXrpvpNQHJJ98Eib4QByoNelIWQiczPqAic2Y2ibYKTURqEzoE55PHgSFq9A/640?wx_fmt=png)



**4. 参考文献**



![](https://mmbiz.qpic.cn/sz_mmbiz_png/j3gficicyOvasOHYuGEic5USJwXGgWfxiaIX2QicricliaaRNdf90kTNddVd7GbF4D4a5orMPiaDkQriaGcTrgWf3dCCGog/640?wx_fmt=png)



<font style="color:rgb(52, 73, 94);"> </font>[分析 transformer 模型的参数量、计算量、中间激活、KV cache](https://zhuanlan.zhihu.com/p/624740065)<font style="color:rgb(52, 73, 94);"> </font>

1.  [【万字长文】LLaMA, ChatGLM, BLOOM 的高效参数微调实践](https://zhuanlan.zhihu.com/p/635710004) 
2.  [FlashAttention:加速计算,节省显存, IO 感知的精确注意力](https://zhuanlan.zhihu.com/p/639228219) 



<font style="color:rgb(52, 73, 94);"></font>

