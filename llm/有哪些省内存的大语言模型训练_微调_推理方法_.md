<font style="color:rgb(52, 73, 94);">大模型（LLMs）现在是 NLP 领域的最主流方法之一了。</font>

<font style="color:rgb(52, 73, 94);">这个趋势带来的主要问题之一，就是大模型的训练/微调/推理需要的内存也越来越多。</font>

<font style="color:rgb(52, 73, 94);">举例来说，即使 RTX 3090 有着 24GB 的 RAM，是除了 A100 之外显存最大的显卡。但使用一块 RTX 3090 依然无法 fp32 精度训练最小号的 LLaMA-6B。</font>

<font style="color:rgb(52, 73, 94);">本文总结一些 Memory-Efficient 的 LLMs 的训练/微调/推理方法，包括：</font>

<font style="color:rgb(52, 73, 94);">● fp16</font>

<font style="color:rgb(52, 73, 94);">● int8</font>

<font style="color:rgb(52, 73, 94);">● LoRA</font>

<font style="color:rgb(52, 73, 94);">● Gradient checkpointing</font>

<font style="color:rgb(52, 73, 94);">● Torch FSDP</font>

<font style="color:rgb(52, 73, 94);">● CPU offloading</font>

### <font style="color:rgb(0, 0, 0);">估算模型所需的RAM</font><font style="color:rgb(63, 63, 63);"></font>
<font style="color:rgb(52, 73, 94);">首先，我们需要了解如何根据参数量估计模型大致所需的 RAM，这在实践中有很重要的参考意义。我们需要通过估算设置 batch_size，设置模型精度，选择微调方法和参数分布方法等。</font>

<font style="color:rgb(52, 73, 94);">接下来，我们用 LLaMA-6B 模型为例估算其大致需要的内存。</font>

<font style="color:rgb(52, 73, 94);">首先考虑精度对所需内存的影响：</font>

<font style="color:rgb(52, 73, 94);">● fp32 精度，一个参数需要 32 bits, 4 bytes.</font>

<font style="color:rgb(52, 73, 94);">● fp16 精度，一个参数需要 16 bits, 2 bytes.</font>

<font style="color:rgb(52, 73, 94);">● int8 精度，一个参数需要 8 bits, 1 byte.</font>

<font style="color:rgb(52, 73, 94);">其次，考虑模型需要的 RAM 大致分三个部分：</font>

<font style="color:rgb(52, 73, 94);">● 模型参数</font>

<font style="color:rgb(52, 73, 94);">● 梯度</font>

<font style="color:rgb(52, 73, 94);">● 优化器参数</font>

<font style="color:rgb(52, 73, 94);">模型参数：等于参数量*每个参数所需内存。</font>

<font style="color:rgb(52, 73, 94);">对于 fp32，LLaMA-6B 需要 6B*4 bytes = 24GB内存</font>

<font style="color:rgb(52, 73, 94);">对于 int8，LLaMA-6B 需要 6B*1 byte = 6GB</font>

<font style="color:rgb(52, 73, 94);">梯度：同上，等于参数量*每个梯度参数所需内存。</font>

<font style="color:rgb(52, 73, 94);">优化器参数：不同的优化器所储存的参数量不同。</font>

<font style="color:rgb(52, 73, 94);">对于常用的 AdamW 来说，需要储存两倍的模型参数（用来储存一阶和二阶momentum）。</font>

<font style="color:rgb(52, 73, 94);">fp32 的 LLaMA-6B，AdamW 需要 6B*8 bytes = 48 GB</font>

<font style="color:rgb(52, 73, 94);">int8 的 LLaMA-6B，AdamW 需要 6B*2 bytes = 12 GB</font>

<font style="color:rgb(52, 73, 94);">除此之外，CUDA kernel 也会占据一些 RAM，大概 1.3GB 左右，查看方式如下。</font>

![](https://cdn.nlark.com/yuque/0/2023/png/35026057/1694081360947-e0e858e4-eb34-4fa5-81f1-d6e0ad9a59c1.png)

<font style="color:rgb(52, 73, 94);">综上，int8 精度的 LLaMA-6B 模型部分大致需要 6GB+6GB+12GB+1.3GB = 25.3GB 左右。</font>

<font style="color:rgb(52, 73, 94);">再根据LLaMA的架构（hidden_size = 4096, intermediate_size =11008, num_hidden_layers = 32, context_length = 2048）计算中间变量内存。</font>

<font style="color:rgb(52, 73, 94);">每个 instance 需要：  
</font>

![](https://cdn.nlark.com/yuque/0/2023/png/35026057/1694081361017-efa9dbc3-aeca-44cb-94aa-3e70b9f0e5d2.png)<font style="color:rgb(52, 73, 94);">所以一张 A100（80GB RAM）大概可以在 int8 精度；batch_size = 50 的设定下进行全参数训练。</font>

<font style="color:rgb(52, 73, 94);">查看消费级显卡的内存和算力：</font>

<font style="color:rgb(52, 73, 94);">2023 GPU Benchmark and Graphics Card Comparison Chart</font>

<font style="color:rgb(52, 73, 94);">https://www.gpucheck.com/gpu-benchmark-graphics-card-comparison-chart</font>

### <font style="color:rgb(0, 0, 0);">Fp16-mixed precision</font><font style="color:rgb(52, 73, 94);">  
</font>
![](https://cdn.nlark.com/yuque/0/2023/png/35026057/1694081360908-4933186e-2f42-43cf-9684-a82835b98578.png)

<font style="color:rgb(52, 73, 94);">混合精度训练的大致思路是在 forward pass 和 gradient computation 的时候使用 fp16 来加速，但是在更新参数时使用 fp32。</font>

<font style="color:rgb(52, 73, 94);">用 torch 实现：</font>

<font style="color:rgb(52, 73, 94);">CUDA Automatic Mixed Precision examples</font>

<font style="color:rgb(52, 73, 94);">https://pytorch.org/docs/stable/notes/amp_examples.html</font>

<font style="color:rgb(52, 73, 94);">torch fp16 推理：直接使用 model.half() 将模型转换为fp16.</font>

![](https://cdn.nlark.com/yuque/0/2023/png/35026057/1694081362731-db3e6c3b-6db7-4b21-a389-f2f8acb5ce2a.png)<font style="color:rgba(0, 0, 0, 0.9);"></font>

<font style="color:rgb(52, 73, 94);">使用 Huggingface Transformers：在 TrainingArguments 里声明 fp16=True</font>

<font style="color:rgb(52, 73, 94);">https://huggingface.co/docs/transformers/perf_train_gpu_one#fp16-training</font>

### <font style="color:rgb(0, 0, 0);">Int8-bitsandbytes</font><font style="color:rgb(63, 63, 63);"></font>
<font style="color:rgb(52, 73, 94);">Int8 是个很极端的数据类型，它最多只能表示 - 128～127 的数字，并且完全没有精度。</font>

<font style="color:rgb(52, 73, 94);">为了在训练和 inference 中使用这个数据类型，bitsandbytes 使用了两个方法最大程度地降低了其带来的误差：</font>

<font style="color:rgb(52, 73, 94);">1. vector-wise quantization</font>

<font style="color:rgb(52, 73, 94);">2. mixed precision decompasition</font>

<font style="color:rgb(52, 73, 94);">Huggingface 在这篇文章中用动图解释了 quantization 的实现：</font>

<font style="color:rgb(52, 73, 94);">https://huggingface.co/blog/hf-bitsandbytes-integration</font>

<font style="color:rgb(52, 73, 94);">论文：</font>

<font style="color:rgb(52, 73, 94);">LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale</font>

<font style="color:rgb(52, 73, 94);">https://arxiv.org/abs/2208.07339</font>

<font style="color:rgb(52, 73, 94);">借助 Huggingface PEFT，使用 int8 训练 opt-6.5B 的完整流程：</font>

<font style="color:rgb(52, 73, 94);">https://github.com/huggingface/peft/blob/main/examples/int8_training/Finetune_opt_bnb_peft.ipynb</font>

### <font style="color:rgb(0, 0, 0);">LoRA</font><font style="color:rgb(63, 63, 63);"></font>
<font style="color:rgb(52, 73, 94);">Low-Rank Adaptation 是微调 LLMs 最常用的省内存方法之一。</font>

![](https://cdn.nlark.com/yuque/0/2023/png/35026057/1694081362823-747882ca-d6ca-41a7-8b32-ed7ba46f640d.png)

<font style="color:rgb(52, 73, 94);">LoRA 发现再微调 LLMs 时，更新矩阵（update matrix）往往特别 sparse，也就是说 update matrix 是低秩矩阵。LoRA 的作者根据这一特点将 update matrix reparametrize 为两个低秩矩阵的积积 。</font>

<font style="color:rgb(52, 73, 94);">如此一来，A+B 的参数量将大大小于 .</font>

<font style="color:rgb(52, 73, 94);">LoRA 的论文：</font>

<font style="color:rgb(52, 73, 94);">https://arxiv.org/pdf/2106.09685.pdf</font>

<font style="color:rgb(52, 73, 94);">借助 Huggingface PEFT 框架，使用 LoRA 微调 mt0：</font>

<font style="color:rgb(52, 73, 94);">https://github.com/huggingface/peft/blob/main/examples/conditional_generation/peft_lora_seq2seq.ipynb</font>

### <font style="color:rgb(0, 0, 0);">Gradient Checkpointing</font>
<font style="color:rgb(52, 73, 94);">在 torch 中使用 - 把 model 用一个 customize 的 function 包装一下即可，详见：</font>

**<font style="color:rgb(0, 0, 0);">Explore Gradient-Checkpointing in PyTorch</font>**

<font style="color:rgb(52, 73, 94);">https://qywu.github.io/2019/05/22/explore-gradient-checkpointing.html</font>

<font style="color:rgb(63, 63, 63);">在 Huggingface Transformers 中使用：</font>

<font style="color:rgb(52, 73, 94);">https://huggingface.co/docs/transformers/v4.27.2/en/perf_train_gpu_one#gradient-checkpointing</font>

### <font style="color:rgb(0, 0, 0);">Torch FSDP+CPU offload</font>
<font style="color:rgb(52, 73, 94);">Fully Sharded Data Paralle（FSDP）和 DeepSpeed 类似，均通过 ZeRO 等分布优化算法，减少内存的占用量。其将模型参数，梯度和优化器状态分布至多个 GPU 上，而非像 DDP 一样，在每个 GPU 上保留完整副本。</font>

<font style="color:rgb(52, 73, 94);">CPU offload 则允许在一个 back propagation 中，将参数动态地从 GPU -> CPU, CPU -> GPU 进行转移，从而节省 GPU 内存。</font>

<font style="color:rgb(52, 73, 94);">Huggingface 这篇博文解释了 ZeRO 的大致实现方法：</font>

<font style="color:rgb(52, 73, 94);">https://huggingface.co/blog/zero-deepspeed-fairscale</font>

<font style="color:rgb(52, 73, 94);">借助 torch 实现 FSDP，只需要将 model 用 FSDPwarp 一下；同样，cpu_offload 也只需要一行代码：</font>

<font style="color:rgb(52, 73, 94);">https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/</font>

<font style="color:rgb(52, 73, 94);">在这个可以查看 FSDP 支持的模型：</font>

<font style="color:rgb(52, 73, 94);">https://pytorch.org/docs/stable/fsdp.html</font>

<font style="color:rgb(52, 73, 94);">在 Huggingface Transformers 中使用 Torch FSDP：</font>

<font style="color:rgb(52, 73, 94);">https://huggingface.co/docs/transformers/v4.27.2/en/main_classes/trainer#transformers.Trainin</font>

<font style="color:rgb(52, 73, 94);">根据某些 issue，shard_grad_op（只分布保存 optimizer states 和 gradients）模式可能比 fully_shard 更稳定：</font>

<font style="color:rgb(52, 73, 94);">https://github.com/tatsu-lab/stanford_alpaca/issues/32</font>

